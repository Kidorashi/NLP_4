{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim, tfidf=False):\n",
    "    text = text.split()\n",
    "    \n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            if tfidf:\n",
    "                vectors[i] = (v * (words[word] / total)) * tfidf[word]\n",
    "            else:\n",
    "                vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Немного парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = html.fromstring(open('paraphrases.xml', 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 =[]\n",
    "text2=[]\n",
    "label =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<paraphrase>\n",
    "      <value name=\"id\">1</value>\n",
    "      <value name=\"id_1\">201</value>\n",
    "      <value name=\"id_2\">8159</value>\n",
    "      <value name=\"text_1\">Полицейским разрешат стрелять на поражение по гражданам с травматикой.</value>\n",
    "      <value name=\"text_2\">Полиции могут разрешить стрелять по хулиганам с травматикой.</value>\n",
    "      <value name=\"jaccard\">0.65</value>\n",
    "      <value name=\"class\">0</value>\n",
    "</paraphrase>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase in corpus.xpath('//paraphrase'):\n",
    "    text1.append(phrase.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    text2.append(phrase.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    label.append(phrase.xpath('./value[@name=\"class\"]/text()')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text_1':text1, 'text_2':text1, 'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['text_1_token'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_token'] = data['text_2'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "      <th>text_1_token</th>\n",
       "      <th>text_2_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>полицейским разрешат стрелять на поражение по ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>право полицейских на проникновение в жилище ре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>президент египет ввести чрезвычайный положение...</td>\n",
       "      <td>президент египет ввести чрезвычайный положение...</td>\n",
       "      <td>президент египта ввел чрезвычайное положение в...</td>\n",
       "      <td>президент египта ввел чрезвычайное положение в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>вернувшихся из сирии россиян волнует вопрос тр...</td>\n",
       "      <td>вернувшихся из сирии россиян волнует вопрос тр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
       "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
       "      <td>в москву из сирии вернулись 2 самолета мчс с р...</td>\n",
       "      <td>в москву из сирии вернулись 2 самолета мчс с р...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             text_1  \\\n",
       "0     0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1     0  Право полицейских на проникновение в жилище ре...   \n",
       "2     0  Президент Египта ввел чрезвычайное положение в...   \n",
       "3    -1  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4     0  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  Право полицейских на проникновение в жилище ре...   \n",
       "2  Президент Египта ввел чрезвычайное положение в...   \n",
       "3  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "2  президент египет ввести чрезвычайный положение...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолёт мчс россиянин...   \n",
       "\n",
       "                                         text_2_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "2  президент египет ввести чрезвычайный положение...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолёт мчс россиянин...   \n",
       "\n",
       "                                        text_1_token  \\\n",
       "0  полицейским разрешат стрелять на поражение по ...   \n",
       "1  право полицейских на проникновение в жилище ре...   \n",
       "2  президент египта ввел чрезвычайное положение в...   \n",
       "3  вернувшихся из сирии россиян волнует вопрос тр...   \n",
       "4  в москву из сирии вернулись 2 самолета мчс с р...   \n",
       "\n",
       "                                        text_2_token  \n",
       "0  полицейским разрешат стрелять на поражение по ...  \n",
       "1  право полицейских на проникновение в жилище ре...  \n",
       "2  президент египта ввел чрезвычайное положение в...  \n",
       "3  вернувшихся из сирии россиян волнует вопрос тр...  \n",
       "4  в москву из сирии вернулись 2 самолета мчс с р...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_learn = pd.read_csv('news_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_learn = data_learn.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Канцлер Германии Ангела Меркель в ходе брифинг...</td>\n",
       "      <td>канцлер германия ангел меркель ход брифинг пре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Российские и белорусские войска успешно заверш...</td>\n",
       "      <td>российский белорусский войско успешно завершит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дзюба, Шатов и Анюков оказались не нужны «Зени...</td>\n",
       "      <td>дзюба шат анюк оказаться нужный зенит российск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Испанию без фанатов\\nПожалуй, главной пятнич...</td>\n",
       "      <td>испания фанат пожалуй главный пятничный новост...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Постпред России при ООН Виталий Чуркин, говоря...</td>\n",
       "      <td>постпред россия оон виталий чуркин говорить ве...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Канцлер Германии Ангела Меркель в ходе брифинг...   \n",
       "1  Российские и белорусские войска успешно заверш...   \n",
       "2  Дзюба, Шатов и Анюков оказались не нужны «Зени...   \n",
       "3  В Испанию без фанатов\\nПожалуй, главной пятнич...   \n",
       "4  Постпред России при ООН Виталий Чуркин, говоря...   \n",
       "\n",
       "                                        content_norm  \n",
       "0  канцлер германия ангел меркель ход брифинг пре...  \n",
       "1  российский белорусский войско успешно завершит...  \n",
       "2  дзюба шат анюк оказаться нужный зенит российск...  \n",
       "3  испания фанат пожалуй главный пятничный новост...  \n",
       "4  постпред россия оон виталий чуркин говорить ве...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_learn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вектора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуйте тексты в векторы в каждой паре 4 методами  - SVD, NMF, Word2Vec, Fastext. Для SVD и NMF сделайте две пары векторов - через TfidfVectorizer и CountVectorizer. Для word2vec сделайте две пары векторов - с взвешиванием по tfidf и без. Для Fastext постройте две модели - без нормализации и с нормализацией, а через каждую модель постройте две пары векторов -  с взвешиванием по tfidf и без. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CountVectorizer(max_features=10000) #5000\n",
    "cv = c.fit(data_learn['content_norm']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TfidfVectorizer(max_features=10000)#5000\n",
    "tv = t.fit(data_learn['content_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_svd_cv = TruncatedSVD(300) #150\n",
    "m_svd_tv = TruncatedSVD(300) #150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_svd_cv.fit(cv)\n",
    "m_svd_tv.fit(tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CountVectoriser\n",
    "a = m_svd_cv.transform(cv.transform(data['text_1_norm']))\n",
    "b = m_svd_cv.transform(cv.transform(data['text_2_norm']))\n",
    "svd_cv = [a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "#TFIDF\n",
    "a = m_svd_tv.transform(tv.transform(data['text_1_norm']))\n",
    "b = m_svd_tv.transform(tv.transform(data['text_2_norm']))\n",
    "svd_tv = [a, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_nmf_cv = NMF(300) #150\n",
    "m_nmf_tv = NMF(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CountVectoriser\n",
    "a = m_nmf_cv.fit_transform(cv.transform(data['text_1_norm']))\n",
    "b = m_nmf_cv.fit_transform(cv.transform(data['text_2_norm']))\n",
    "nmf_cv = [a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "a = m_nmf_tv.fit_transform(tv.transform(data['text_1_norm']))\n",
    "b = m_nmf_tv.fit_transform(tv.transform(data['text_2_norm']))\n",
    "nmf_tv = [a, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec([text.split() for text in data_learn['content_norm'].values], size=300, sg=1) #150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#без TFIDF\n",
    "dim = 300 #150\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_notv = [X_text_1_w2v, X_text_1_w2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#c TFIDF\n",
    "t = TfidfVectorizer(max_features=5000)\n",
    "tv_dop = t.fit_transform(data_learn['content_norm'])\n",
    "\n",
    "dim = 300 #150\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "# t1 = t.fit(X_text_1_w2v).vocabulary_\n",
    "# t2 = t.fit(X_text_2_w2v).vocabulary_\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim, tv_dop)\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim, tv_dop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_tv = [X_text_1_w2v, X_text_1_w2v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_norm = gensim.models.FastText([text.split() for text in data_learn['content_norm'].values], \n",
    "                              size=150, min_n=4, max_n=8)\n",
    "ft = gensim.models.FastText([text.split() for text in data_learn['content'].values], \n",
    "                              size=150, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#norm, TFIDF\n",
    "dim = 300 #150\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim, tv_dop)\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim, tv_dop)\n",
    "    \n",
    "ft_n_tv = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#norm, no TFIDF\n",
    "dim = 300 #150\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "ft_n = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#not norm, TFIDF\n",
    "dim = 300 #150\n",
    "X_text_1_ft = np.zeros((len(data['text_1_token']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_token']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim, tv_dop)\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim, tv_dop)\n",
    "    \n",
    "ft_tv = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#not norm, no TFIDF\n",
    "dim = 300 #150\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "ft = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Косинусная близость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_dist(vecs):\n",
    "    vec_1 = vecs[0]\n",
    "    vec_2 = vecs[1]\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(vec_1)):\n",
    "        cos_dis = cosine_distances(vec_1[i].reshape(1, -1), vec_2[i].reshape(1, -1))\n",
    "        result.append(cos_dis[0][0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = cos_dist(svd_tv)\n",
    "results = pd.DataFrame({'svd_tv':res})\n",
    "res = cos_dist(svd_cv)\n",
    "results['svd_cv'] = res\n",
    "\n",
    "res = cos_dist(nmf_tv)\n",
    "results['nmf_cv'] = res\n",
    "res = cos_dist(nmf_cv)\n",
    "results['nmf_cv'] = res\n",
    "\n",
    "res = cos_dist(w2v_tv)\n",
    "results['w2v_tv'] = res\n",
    "res = cos_dist(w2v_notv)\n",
    "results['w2v_notv'] = res\n",
    "\n",
    "res = cos_dist(ft_n_tv)\n",
    "results['ft_n_tv'] = res\n",
    "res = cos_dist(ft_n)\n",
    "results['ft_n'] = res\n",
    "res = cos_dist(ft_tv)\n",
    "results['ft_tv'] = res\n",
    "res = cos_dist(ft)\n",
    "results['ft'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_tv</th>\n",
       "      <th>svd_cv</th>\n",
       "      <th>nmf_cv</th>\n",
       "      <th>w2v_tv</th>\n",
       "      <th>w2v_notv</th>\n",
       "      <th>ft_n_tv</th>\n",
       "      <th>ft_n</th>\n",
       "      <th>ft_tv</th>\n",
       "      <th>ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.790547</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.054093</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.480549</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.440892e-16</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         svd_tv        svd_cv    nmf_cv        w2v_tv      w2v_notv  ft_n_tv  \\\n",
       "0  0.000000e+00  2.220446e-16  0.790547  0.000000e+00  0.000000e+00      1.0   \n",
       "1  2.220446e-16  3.330669e-16  0.054093  2.220446e-16  2.220446e-16      1.0   \n",
       "2  0.000000e+00  0.000000e+00  0.480549  0.000000e+00  0.000000e+00      1.0   \n",
       "3  0.000000e+00  0.000000e+00  0.006462  0.000000e+00  0.000000e+00      1.0   \n",
       "4  0.000000e+00  4.440892e-16  0.003660  3.330669e-16  3.330669e-16      1.0   \n",
       "\n",
       "   ft_n  ft_tv   ft  \n",
       "0   1.0    1.0  1.0  \n",
       "1   1.0    1.0  1.0  \n",
       "2   1.0    1.0  1.0  \n",
       "3   1.0    1.0  1.0  \n",
       "4   1.0    1.0  1.0  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_trai, y_test = train_test_split(results, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4047373954157553"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=100, class_weight='balanced', max_iter=1500, n_jobs=-1)\n",
    "cross_val_score(clf, results, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перебор параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я попыталсь увеличить некоторые параметры. Но что-то пошло не так. Нули в FastText были и изначально, я так и не смогла понять, что не так."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
